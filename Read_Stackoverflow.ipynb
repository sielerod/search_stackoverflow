{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sielerod/search_stackoverflow/blob/master/Read_Stackoverflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhI60Te7lUP1",
        "colab_type": "text"
      },
      "source": [
        "#**Objetivo:**   \n",
        "* Capturar as perguntas mais frequentes sobre Python no stackoverflow\n",
        "* Armazenar para cada pergunta: link, breve descrição da pergunta, quantidade de votos e visualizações, pergunta, respostas com melhor avaliação\n",
        "\n",
        "\n",
        "**Fonte:** https://stackoverflow.com/questions/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTKKIbRrd_fr",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import requests # Coleta de conteúdo em Webpage\n",
        "from requests.exceptions import HTTPError\n",
        "from bs4 import BeautifulSoup as bs # Scraping webpages\n",
        "from time import sleep\n",
        "import json\n",
        "\n",
        "import re #biblioteca para trabalhar com regular expressions - regex\n",
        "import string\n",
        "import unidecode\n",
        "\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#from nltk.stem import RSLPStemmer #Stemming Portugues\n",
        "#from nltk.stem import PorterStemmer #Stemming Ingles com algoritmo de Porter: algoritmo menos agressivo nas reduções\n",
        "from nltk.stem import SnowballStemmer #Stemming Porter2: mais agressivo nas reduções do que Porter stemmer e um pouco mais rápido \n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>|&[.*?]')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pe2opv4SYoR",
        "colab_type": "text"
      },
      "source": [
        "#Leitura do dado cru no Stackoverflow\n",
        "**read_stackoverflow_overview(tags=[], tab='Frequent', pages)**\n",
        "\n",
        "Leitura do resumo das perguntas mais frequentes no stackoverflow com base em alguns parâmetros de busca. \n",
        "\n",
        "Retorna um objeto requests contendo o resultado de requests.get\n",
        "\n",
        "* tags: argumento opcional com lista  de strings contendo os tipos de pergunta para seleação. Ex.: ['python', 'php', 'javascript']\n",
        ">ex. de URL para página com mais de 1 tag: https://stackoverflow.com/questions/tagged/sql+sql-server?tab=Frequent\n",
        "\n",
        "* tab: string com tipo de ordenação a ser aplicado, pode ser:\n",
        "'Frequent' (opção default), 'Votes', 'Unanswered', 'Bounties', 'Active', 'Newest'\n",
        "\n",
        "* Selector: seleção dos trechos do html a serem retornados. Por default, será question-summary\n",
        "\n",
        "* pages: número de páginas para leitura\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYnP7RwH0h9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_stackoverflow_overview(tags=[], tab='Frequent', pages=5):\n",
        "  link = 'https://stackoverflow.com/questions'\n",
        "  selector='question-summary'\n",
        "  \n",
        "  if tags:\n",
        "    tags_link = '/tagged/'\n",
        "    pre=''\n",
        "    for t in tags:\n",
        "      tags_link += pre + t\n",
        "      pre = '+' \n",
        "    link += tags_link\n",
        "\n",
        "  link += '?tab='+tab\n",
        "\n",
        "  questions_text = ''\n",
        "  soup_selection = []\n",
        "  for page in range(1,pages+1):\n",
        "    page_link = '&page='+str(page)\n",
        "\n",
        "    try:\n",
        "      request = requests.get(link+page_link)\n",
        "      request.raise_for_status()\n",
        "      try:\n",
        "        soup = bs(request.text, 'html.parser')\n",
        "        soup_selection.append(soup.select('.'+selector))\n",
        "      except: print (\"Could not transform to soup object by selecting \",selector)\n",
        "    except HTTPError:\n",
        "      print (\"Could not download page \", page)\n",
        "\n",
        "    sleep(0.05)\n",
        "\n",
        "  return soup_selection\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "list"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "questions_overview_raw = read_stackoverflow_overview(tags=['python','django'],tab='Frequent',pages=2)\n",
        "type(questions_overview_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21u58D6xwFkU",
        "colab_type": "text"
      },
      "source": [
        "#Transformação do dado cru coletado do Stackoverflow em dataframe\n",
        "**questions_overview(questions_overview_raw)**\n",
        "\n",
        "O dataframe deve conter a visão geral das perguntas do stackoverflow, com:\n",
        "\n",
        "* link\n",
        "* brief_description\n",
        "* votes\n",
        "* views"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fj83JnZmFtH",
        "colab_type": "text"
      },
      "source": [
        "###Análise do padrão da página HTML para captura de informações relevantes:\n",
        "\n",
        "Em \"question-summary\", temos as seguintes informações relevantes:\n",
        "\n",
        "1.   class = statscontainer, com:\n",
        "*   Número de votos em class=\"vote-count-post \"\n",
        ">```<span class=\"vote-count-post high-scored-post\"><strong>2473</strong></span>```\n",
        "\n",
        "*   Número de respostas aceitas em class=\"status answered-accepted\" \n",
        ">```<div class=\"status answered-accepted\"><strong>23</strong>answers</div>```\n",
        "\n",
        "*   Conteúdo e Title contendo quantidade de views em class=\"views supernova\" \n",
        ">```<div class=\"views supernova\" title=\"307,292 views\">307k views</div>```\n",
        "\n",
        "2.   class = summary, com:\n",
        "* class=\"question-hyperlink\" contendo em *href* parte do link para compor link de acesso à página detalhada da pergunta e Título da pergunta\n",
        ">``` <a href=\"/questions/15112125/how-to-test-multiple-variables-against-a-value\" class=\"question-hyperlink\">How to test multiple variables against a value?</a>```\n",
        "\n",
        "*   Breve resumo em class=\"excerpt\"\n",
        ">```<div class=\"excerpt\"> brief description of the question ...</div>```\n",
        "\n",
        "*   Tags em class=\"post-tag\"\n",
        ">```<a href=\"/questions/tagged/python\" class=\"post-tag\" title=\"show questions tagged 'python'\" rel=\"tag\">python</a>```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEQEhGyPCfX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def questions_overview(questions_overview_raw):\n",
        "  questions_overview = { 'questions':[]}\n",
        "\n",
        "  for soups in questions_overview_raw:\n",
        "    for soup in soups:\n",
        "      title = soup.select_one('.question-hyperlink').getText()\n",
        "      link = 'https://stackoverflow.com'+soup.select_one('.question-hyperlink').get('href')\n",
        "      summary = soup.select_one('.excerpt').getText()\n",
        "      vote_count =  soup.select_one('.vote-count-post').getText()\n",
        "      answers_count = soup.select_one('.answered-accepted')\n",
        "      answers_count = re.sub('\\D','',answers_count.getText('')) if answers_count else '0'\n",
        "      views =  re.sub('views','',soup.select_one('.views').attrs['title'])\n",
        "      views = re.sub(',','',views)\n",
        "      tags = []\n",
        "      for tag in soup.select('.post-tag'): tags.append(tag.getText())\n",
        "\n",
        "      questions_overview['questions'].append({\n",
        "          'title': title,\n",
        "          'link': link,\n",
        "          'summary': summary,\n",
        "          'vote_count': int(vote_count),\n",
        "          'answers_count': int(answers_count),\n",
        "          'views': int(views),\n",
        "          'tags': tags,\n",
        "          'full_question': '',\n",
        "          'best_answer': '',\n",
        "      })\n",
        "\n",
        "  questions_df = pd.DataFrame(questions_overview['questions'])\n",
        "  \n",
        "  return questions_df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPT8KArDl4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9dc45e4-1fdd-4a17-e462-6cbc70c598a9",
        "tags": []
      },
      "source": [
        "questions_df = questions_overview(questions_overview_raw)\n",
        "type(questions_df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "pandas.core.frame.DataFrame"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feA0REYGN3cg",
        "colab_type": "text"
      },
      "source": [
        "#Exemplos de como acessar a informação no dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhPPA_TmYceK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "5495d8d7-1bef-407f-c954-26d4c19ca3a9",
        "tags": []
      },
      "source": [
        "print('Lista com links:\\n',questions_df['link'][0:3])\n",
        "print('\\n Acesso a dados de um link específico\\n--- Link: ',questions_df['link'][0])\n",
        "print('\\n--- Título: ', questions_df['title'][0])\n",
        "print('\\n--- Breve Descrição: ', questions_df['summary'][0])\n",
        "print('\\n--- Contagem de votos: ', questions_df['vote_count'][0])\n",
        "print('\\n--- Contagem de respostas: ', questions_df['answers_count'][0])\n",
        "print('\\n--- Contagem de visualizações: ', questions_df['views'][0])\n",
        "print('\\n--- Lista como tags: ', questions_df['tags'][0])\n",
        "questions_df.head(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Lista com links:\n 0    https://stackoverflow.com/questions/23708898/p...\n1    https://stackoverflow.com/questions/573618/set...\n2    https://stackoverflow.com/questions/8000022/dj...\nName: link, dtype: object\n\n Acesso a dados de um link específico\n--- Link:  https://stackoverflow.com/questions/23708898/pip-is-not-recognized-as-an-internal-or-external-command\n\n--- Título:  'pip' is not recognized as an internal or external command\n\n--- Breve Descrição:  \n            I'm running into a weird error when trying to install Django on my computer.\nThis is the sequence that I typed into my command line:\nC:\\Python34> python get-pip.py\nRequirement already up-to-date: ...\n        \n\n--- Contagem de votos:  343\n\n--- Contagem de respostas:  32\n\n--- Contagem de visualizações:  1060830\n\n--- Lista como tags:  ['python', 'django', 'windows', 'pip']\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   answers_count best_answer full_question  \\\n0             32                             \n1             24                             \n2              8                             \n\n                                                link  \\\n0  https://stackoverflow.com/questions/23708898/p...   \n1  https://stackoverflow.com/questions/573618/set...   \n2  https://stackoverflow.com/questions/8000022/dj...   \n\n                                             summary  \\\n0  \\r\\n            I'm running into a weird error...   \n1  \\r\\n            I've been working on a web app...   \n2  \\r\\n            mydict = {\"key1\":\"value1\", \"ke...   \n\n                                                tags  \\\n0                     [python, django, windows, pip]   \n1  [python, django, web-applications, scheduled-t...   \n2            [python, django, templates, dictionary]   \n\n                                               title    views  vote_count  \n0  'pip' is not recognized as an internal or exte...  1060830         343  \n1                            Set up a scheduled job?   170303         523  \n2  Django template how to look up a dictionary va...   143481         236  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answers_count</th>\n      <th>best_answer</th>\n      <th>full_question</th>\n      <th>link</th>\n      <th>summary</th>\n      <th>tags</th>\n      <th>title</th>\n      <th>views</th>\n      <th>vote_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td></td>\n      <td></td>\n      <td>https://stackoverflow.com/questions/23708898/p...</td>\n      <td>\\r\\n            I'm running into a weird error...</td>\n      <td>[python, django, windows, pip]</td>\n      <td>'pip' is not recognized as an internal or exte...</td>\n      <td>1060830</td>\n      <td>343</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24</td>\n      <td></td>\n      <td></td>\n      <td>https://stackoverflow.com/questions/573618/set...</td>\n      <td>\\r\\n            I've been working on a web app...</td>\n      <td>[python, django, web-applications, scheduled-t...</td>\n      <td>Set up a scheduled job?</td>\n      <td>170303</td>\n      <td>523</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td></td>\n      <td></td>\n      <td>https://stackoverflow.com/questions/8000022/dj...</td>\n      <td>\\r\\n            mydict = {\"key1\":\"value1\", \"ke...</td>\n      <td>[python, django, templates, dictionary]</td>\n      <td>Django template how to look up a dictionary va...</td>\n      <td>143481</td>\n      <td>236</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rFjl4HKL2is",
        "colab_type": "text"
      },
      "source": [
        "Próximos passos:\n",
        "\n",
        "\n",
        "1.   Enriquecer questions_df com a informação detalhada da pergunta e conteúdo da resposta com melhor avaliação\n",
        "2.   Limpar dados em questions_df para remover caracteres irrelevantes, como: \\n, \\t, artigos, pronomes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pncNTKawYOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_question_detail(questions_df):\n",
        "  \n",
        "  idx = 0\n",
        "  for link in questions_df['link']:\n",
        "    question = []\n",
        "    answer = []\n",
        "    try:\n",
        "      request = requests.get(link)\n",
        "      request.raise_for_status()\n",
        "      try:\n",
        "        soup = bs(request.text, 'html.parser')\n",
        "        questions_df['full_question'][idx] = soup.find(\"div\", {\"id\": \"question\"}).select_one('.post-text').getText()\n",
        "        questions_df['best_answer'][idx] = soup.find(\"div\", {\"id\": \"answers\"}).select_one('.post-text').getText()\n",
        "\n",
        "      except: \n",
        "        print (\"Could not transform to soup object by selecting\")\n",
        "\n",
        "    except HTTPError:\n",
        "      print (\"Could not download page\")\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "    sleep(0.05)\n",
        "\n",
        "  return questions_df"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCsMerWlwhjU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "ca1ec92b-1fa9-41ac-bf90-c9e99905c553",
        "tags": []
      },
      "source": [
        "questions_df = read_question_detail(questions_df)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Index(['answers_count', 'best_answer', 'full_question', 'link', 'summary',\n       'tags', 'title', 'views', 'vote_count'],\n      dtype='object')"
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "questions_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove todas as pontuações e retorna lista de palavras\n",
        "def clean_text (text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) #remove todas as pontuações: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    text = text.replace('\\n',' ').strip() \n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "#redução das palavras para sua raiz (stemming), remoção de stopwords e palavras com menos de 2 caracteres, e criação do vocabulário com a quantidade de ocorrência de cada palavra em todos os documentos\n",
        "\n",
        "def stackoverflow_vocabulary(questions_df):\n",
        "    docs_stem_words = []\n",
        "    vocabulary = {}\n",
        "    stop_words = stopwords.words('english')\n",
        "    snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "    for index in range(len(questions_df)):\n",
        "        text = questions_df['title'][index] + questions_df['full_question'][index] + questions_df['best_answer'][index] \n",
        "        tokentext = word_tokenize(clean_text(text))\n",
        "        stem_words  = [snowball_stemmer.stem(word) for word in tokentext if not word in stop_words and len(word) > 2 and word not in string.punctuation]\n",
        "        docs_stem_words.append(stem_words)\n",
        "\n",
        "        #Inicializa vocabulário sem repetição de palavras\n",
        "        for word in stem_words:\n",
        "            vocabulary[word] = 0\n",
        "\n",
        "    #Contabiliza ocorrência de cada palavra em todos os documentos\n",
        "    for words in docs_stem_words:\n",
        "        for word in words:\n",
        "            vocabulary[word] += 1\n",
        "    \n",
        "    return vocabulary, docs_stem_words\n",
        "\n",
        "vocabulary, docs_stem_words = stackoverflow_vocabulary(questions_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Criar índice invertido para viabilizar buscas\n",
        "def create_InvertedIndex(vocabulary, docs_stem_words): \n",
        "    invertedList = dict()\n",
        "    for term in vocabulary:\n",
        "        invertedList[term] = list()\n",
        "        index = 0\n",
        "        for stem_words in docs_stem_words:\n",
        "            frequencia = 0\n",
        "            for word in stem_words:\n",
        "                if word == term:\n",
        "                    frequencia += 1\n",
        "            if frequencia > 0:\n",
        "                invertedList[term].append([index, frequencia])\n",
        "            index += 1\n",
        "            invertedList[term].sort(key=itemgetter(1), reverse=True)\n",
        "\n",
        "    # Serialize data into file:\n",
        "    json.dump(invertedList, open( \"stackoverflow_InvertedIndex.json\", 'w' ) )\n",
        "\n",
        "    return #invertedList\n",
        "\n",
        "#invertedList = create_InvertedIndex(vocabulary, docs_stem_words)\n",
        "create_InvertedIndex(vocabulary, docs_stem_words)\n",
        "\n",
        "# Read data from file:\n",
        "invertedList = json.load( open( \"stackoverflow_InvertedIndex.json\" ) )\n",
        "#invertedList.items()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[[69, 10], [9, 7], [70, 6], [73, 6], [31, 5]]"
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "invertedList['python'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_stemming_docs(documents):\n",
        "    snowball_stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens = sum([word_tokenize(clean_text(document)) for document in documents], [])\n",
        "    stem_words  = [snowball_stemmer.stem(word) for word in tokens if not word in stop_words and len(word) > 2 and word not in string.punctuation]\n",
        "\n",
        "    return stem_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_lookup_query(query, invertedList):\n",
        "    terms = simple_stemming_docs([query])\n",
        "\n",
        "    docs_index = {}\n",
        "\n",
        "    for term in terms:\n",
        "        if term in invertedList.keys():\n",
        "            docs_index[term] = [index[0] for index in invertedList[term]]\n",
        "        else:\n",
        "            docs_index['missingTerm'] = ['']\n",
        "\n",
        "    return docs_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stemming_docs' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-39-f8f3eb538e52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msearchTerms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Digite os termos de busca: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdocs_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_lookup_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minvertedList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-38-f9f8c0d62b37>\u001b[0m in \u001b[0;36msimple_lookup_query\u001b[1;34m(query, invertedList)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msimple_lookup_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvertedList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemming_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdocs_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'stemming_docs' is not defined"
          ]
        }
      ],
      "source": [
        "searchTerms = input(\"Digite os termos de busca: \")\n",
        "docs_index = simple_lookup_query(searchTerms,invertedList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_search_result(docs_index, docs, operator='OR'):\n",
        "    for i, (k, v) in enumerate(docs_index.items()):\n",
        "        print(\"{:<8}key: {:<20} value: {}\".format(i, k, v))\n",
        "\n",
        "    print()\n",
        "\n",
        "    resultList=[lista[1] for lista in docs_index.items()]\n",
        "\n",
        "    responseSet = []\n",
        "\n",
        "    if operator == 'AND' and [''] in resultList:\n",
        "        resultList = []\n",
        "    elif [''] in resultList:\n",
        "        resultList.remove([''])\n",
        "    \n",
        "    if len(resultList) == 1:\n",
        "        responseSet = resultList[0]\n",
        "\n",
        "    #Realiza a interseção entre os conjuntos\n",
        "    for i in range(len(resultList)-1):\n",
        "        #Operador AND\n",
        "        if operator == 'AND':\n",
        "            responseSet.append(list(set(resultList[i]).intersection(resultList[i+1])))\n",
        "        else:\n",
        "            #Operador OR\n",
        "            responseSet.append(list(set(resultList[i]).union(resultList[i+1])))\n",
        "\n",
        "    print(\"Foram encontrados \", len(np.unique(responseSet)), \" documentos com o termo de busca...\")\n",
        "    print()\n",
        "\n",
        "    #Monta o Resultado\n",
        "    lista = []\n",
        "    for doc in np.unique(responseSet):\n",
        "        documento = docs[doc]\n",
        "        for term in docs_index.keys():\n",
        "            documento = documento.replace(term, \"\\033[48;5;0m\\033[38;5;226m {term} \\033[0;0m\".format(term=term))        \n",
        "        lista.append(str(str(doc+1) + \" - \" + documento))\n",
        "        \n",
        "    #Exibe o Resultado\n",
        "    for resultado in lista:\n",
        "        print(resultado)\n",
        "\n",
        "    return\n",
        "\n",
        "print_search_result(docs_index,documents,'OR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create functions for TD-IDF / BM25\n",
        "import math\n",
        "from textblob import TextBlob as tb\n",
        "\n",
        "def tf(word, doc):\n",
        "    return doc.count(word) / len(doc)\n",
        "\n",
        "def n_containing(word, doclist):\n",
        "    return sum(1 for doc in doclist if word in doc)\n",
        "\n",
        "def idf(word, doclist):\n",
        "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
        "\n",
        "def tfidf(word, doc, doclist):\n",
        "    return (tf(word, doc) * idf(word, doclist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF\n",
        "\n",
        "worddic = {}\n",
        "\n",
        "for index in range(len(set_stem_words)):\n",
        "    for word in wordsunique:\n",
        "        if word in set_stem_words[0][index]:\n",
        "            word = str(word)\n",
        "            positions = list(np.where(np.array(set_stem_words[0][index]) == word)[0])\n",
        "            idfs = tfidf(word,set_stem_words[0][index],set_stem_words)\n",
        "\n",
        "            try:\n",
        "                worddic[word] = [index,positions,idfs]\n",
        "            except:\n",
        "                worddic[word] = []\n",
        "                worddic[word] = [index,positions,idfs]\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[84,\n [6, 44, 56, 60, 64, 86, 99, 102, 108, 111, 183, 186, 204, 217, 227],\n 0.5233147938622832]"
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "worddic['instal']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nI want users on the site to be able to download files whose paths are obscured so they cannot be directly downloaded.\nFor instance, I'd like the URL to be something like this: http://example.com/download/?f=somefile.txt\nAnd on the server, I know that all downloadable files reside in the folder /home/user/files/.\nIs there a way to make Django serve that file for download as opposed to trying to find a URL and View to display it?\n\n"
        }
      ],
      "source": [
        "print(questions_df['full_question'][5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nFor the \"best of both worlds\" you could combine S.Lott's solution with the xsendfile module: django generates the path to the file (or the file itself), but the actual file serving is handled by Apache/Lighttpd. Once you've set up mod_xsendfile, integrating with your view takes a few lines of code:\nfrom django.utils.encoding import smart_str\n\nresponse = HttpResponse(mimetype='application/force-download') # mimetype is replaced by content_type for django 1.7\nresponse['Content-Disposition'] = 'attachment; filename=%s' % smart_str(file_name)\nresponse['X-Sendfile'] = smart_str(path_to_file)\n# It's usually a good idea to set the 'Content-Length' header too.\n# You can also set any other required headers: Cache-Control, etc.\nreturn response\n\nOf course, this will only work if you have control over your server, or your hosting company has mod_xsendfile already set up.\nEDIT:\n\nmimetype is replaced by content_type for django 1.7\n\nresponse = HttpResponse(content_type='application/force-download')  \n\nEDIT:\n For nginx check this, it uses X-Accel-Redirect instead of apache X-Sendfile header.\n\n"
        }
      ],
      "source": [
        "print(questions_df['best_answer'][5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(1, 1)\n<class 'scipy.sparse.csr.csr_matrix'>\n[[1.]]\n"
        }
      ],
      "source": [
        "\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "tfidfvectorizer.fit(['django'])\n",
        "vectortfidf = tfidfvectorizer.transform(['django'])\n",
        "# summarize encoded vector\n",
        "print(vectortfidf.shape)\n",
        "print(type(vectortfidf))\n",
        "print(vectortfidf.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "running weird error trying install django computer sequence typed command line cpython34 python getpippy requirement already uptodate pip cpython34libsitepackages cleaning cpython34 pip install django pip recognized internal external command operable program batch file cpython34 libsitepackagespip install django libsitepackagespip recognized internal external command operable program batch file could causing get type echo path cpython34echo path cprogram filesimagemagick688q16cprogram files x86intelicls client cprogram filesintelicls clientcwindowssystem32cwindowscwindowss ystem32wbemcwindowssystem32windowspowershellv10cprogram files x86 windows livesharedcprogram files x86intelopencl sdk20binx86cprogr files x86intelopencl sdk20binx64cprogram filesintelintelr mana gement engine componentsdalcprogram filesintelintelr management engine omponentsiptcprogram files x86intelintelr management engine components dalcprogram files x86intelintelr management engine componentsiptcp rogram files x86nodejscprogram files x86herokubincprogram files 86gitcmdcrailsinstallerruby200bincrailsinstallergitcmdcrailsin stallerruby193bincusersjaviappdataroamingnpmyou need add path pip installation path system variable default pip installed cpython34scriptspip pip comes bundled new versions python path cpython34scripts needs added path variable check already path variable type echo path cmd prompt add path pip installation path variable use control panel setx command example setx path pathcpython34scripts note according official documentation variables set setx variables available future command windows current command window particular need start new cmdexe instance entering command order utilize new environment variable thanks scott bartell pointing outsuccess success success\n"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer #TF\n",
        "\n",
        "text = ' '.join([word for word in keywords])\n",
        "print(text)\n",
        "\n",
        "set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(1, 113)\n<class 'scipy.sparse.csr.csr_matrix'>\n[[ 1  1  2  1  2  1  1  2  1  1  1  1  1  1  1  1  1  7  1  1  1  1  1  1\n   2  3  1  1  1  1  1  1  1  3  1  2  4  1  1  1  1  2  2  9  1  1  2  1\n   1  1  1  3  2  1  1  2  2  1  1  1  3  2  1  3  1  1  1  2  1  1  1  1\n  11  1  7  1  2  1  2  2  1  1  1  1  1  1  1  1  3  1  1  2  1  1  1  2\n   1  1  1  1  5  2  1  1  1  2  1  1  1  2  2  1  1]]\n"
        }
      ],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit([text])\n",
        "vector = vectorizer.transform([text])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0]]\n"
        }
      ],
      "source": [
        "\n",
        "#Vetoriza um texto novo\n",
        "text2 = [\"How can I install install install install  install Django? No success success success with django so far... django django\"]\n",
        "vector2 = vectorizer.transform(text2)\n",
        "print(vector2.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(1, 113)\n<class 'scipy.sparse.csr.csr_matrix'>\n[[0.04218245 0.04218245 0.08436491 0.04218245 0.08436491 0.04218245\n  0.04218245 0.08436491 0.04218245 0.04218245 0.04218245 0.04218245\n  0.04218245 0.04218245 0.04218245 0.04218245 0.04218245 0.29527718\n  0.04218245 0.04218245 0.04218245 0.04218245 0.04218245 0.04218245\n  0.08436491 0.12654736 0.04218245 0.04218245 0.04218245 0.04218245\n  0.04218245 0.04218245 0.04218245 0.12654736 0.04218245 0.08436491\n  0.16872982 0.04218245 0.04218245 0.04218245 0.04218245 0.08436491\n  0.08436491 0.37964209 0.04218245 0.04218245 0.08436491 0.04218245\n  0.04218245 0.04218245 0.04218245 0.12654736 0.08436491 0.04218245\n  0.04218245 0.08436491 0.08436491 0.04218245 0.04218245 0.04218245\n  0.12654736 0.08436491 0.04218245 0.12654736 0.04218245 0.04218245\n  0.04218245 0.08436491 0.04218245 0.04218245 0.04218245 0.04218245\n  0.46400699 0.04218245 0.29527718 0.04218245 0.08436491 0.04218245\n  0.08436491 0.08436491 0.04218245 0.04218245 0.04218245 0.04218245\n  0.04218245 0.04218245 0.04218245 0.04218245 0.12654736 0.04218245\n  0.04218245 0.08436491 0.04218245 0.04218245 0.04218245 0.08436491\n  0.04218245 0.04218245 0.04218245 0.04218245 0.21091227 0.08436491\n  0.04218245 0.04218245 0.04218245 0.08436491 0.04218245 0.04218245\n  0.04218245 0.08436491 0.08436491 0.04218245 0.04218245]]\n"
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "tfidfvectorizer.fit([text])\n",
        "vectortfidf = tfidfvectorizer.transform([text])\n",
        "# summarize encoded vector\n",
        "print(vectortfidf.shape)\n",
        "print(type(vectortfidf))\n",
        "print(vectortfidf.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.56568542 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.70710678 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.42426407 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.        ]]\n"
        }
      ],
      "source": [
        "\n",
        "#Vetoriza um texto novo\n",
        "vectortfidf2 = tfidfvectorizer.transform(text2)\n",
        "print(vectortfidf2.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "TF  [[0.1968615]]\nTF-IDF:  [[0.1968615]]\n"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(\"TF \", cosine_similarity(vector, vector2))\n",
        "print(\"TF-IDF: \", cosine_similarity(vectortfidf, vectortfidf2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions_overview_raw = read_stackoverflow_overview(tags=['python','django'],tab='Frequent',pages=2)\n",
        "type(questions_overview_raw)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Read_Stackoverflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmUrBHkx90se+IXEtbfA+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}