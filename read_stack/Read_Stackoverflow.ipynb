{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sielerod/search_stackoverflow/blob/master/Read_Stackoverflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhI60Te7lUP1",
        "colab_type": "text"
      },
      "source": [
        "#**Objetivo:**   \n",
        "* Capturar as perguntas mais frequentes sobre Python no stackoverflow\n",
        "* Armazenar para cada pergunta: link, breve descrição da pergunta, quantidade de votos e visualizações, pergunta, respostas com melhor avaliação\n",
        "\n",
        "\n",
        "**Fonte:** https://stackoverflow.com/questions/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTKKIbRrd_fr",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import requests # Coleta de conteúdo em Webpage\n",
        "from requests.exceptions import HTTPError\n",
        "from bs4 import BeautifulSoup as bs # Scraping webpages\n",
        "from time import sleep\n",
        "import json\n",
        "\n",
        "import re #biblioteca para trabalhar com regular expressions - regex\n",
        "import string\n",
        "import unidecode\n",
        "\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "#from nltk.stem import RSLPStemmer #Stemming Portugues\n",
        "#from nltk.stem import PorterStemmer #Stemming Ingles com algoritmo de Porter: algoritmo menos agressivo nas reduções\n",
        "from nltk.stem import SnowballStemmer #Stemming Porter2: mais agressivo nas reduções do que Porter stemmer e um pouco mais rápido \n",
        "#from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanhtml(raw_html):\n",
        "  cleanr = re.compile('<.*?>|&[.*?]')\n",
        "  cleantext = re.sub(cleanr, '', raw_html)\n",
        "  return cleantext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pe2opv4SYoR",
        "colab_type": "text"
      },
      "source": [
        "#Leitura do dado cru no Stackoverflow\n",
        "**read_stackoverflow_overview(tags=[], tab='Frequent', pages)**\n",
        "\n",
        "Leitura do resumo das perguntas mais frequentes no stackoverflow com base em alguns parâmetros de busca. \n",
        "\n",
        "Retorna um objeto requests contendo o resultado de requests.get\n",
        "\n",
        "* tags: argumento opcional com lista  de strings contendo os tipos de pergunta para seleação. Ex.: ['python', 'php', 'javascript']\n",
        ">ex. de URL para página com mais de 1 tag: https://stackoverflow.com/questions/tagged/sql+sql-server?tab=Frequent\n",
        "\n",
        "* tab: string com tipo de ordenação a ser aplicado, pode ser:\n",
        "'Frequent' (opção default), 'Votes', 'Unanswered', 'Bounties', 'Active', 'Newest'\n",
        "\n",
        "* Selector: seleção dos trechos do html a serem retornados. Por default, será question-summary\n",
        "\n",
        "* pages: número de páginas para leitura\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYnP7RwH0h9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_stackoverflow_overview(tags=[], tab='Frequent', pages=5):\n",
        "  link = 'https://stackoverflow.com/questions'\n",
        "  selector='question-summary'\n",
        "  \n",
        "  if tags:\n",
        "    tags_link = '/tagged/'\n",
        "    pre=''\n",
        "    for t in tags:\n",
        "      tags_link += pre + t\n",
        "      pre = '+' \n",
        "    link += tags_link\n",
        "\n",
        "  link += '?tab='+tab\n",
        "\n",
        "  questions_text = ''\n",
        "  soup_selection = []\n",
        "  for page in range(1,pages+1):\n",
        "    page_link = '&page='+str(page)\n",
        "\n",
        "    try:\n",
        "      request = requests.get(link+page_link)\n",
        "      request.raise_for_status()\n",
        "      try:\n",
        "        soup = bs(request.text, 'html.parser')\n",
        "        soup_selection.append(soup.select('.'+selector))\n",
        "      except: print (\"Could not transform to soup object by selecting \",selector)\n",
        "    except HTTPError:\n",
        "      print (\"Could not download page \", page)\n",
        "\n",
        "    sleep(0.05)\n",
        "\n",
        "  return soup_selection\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "list"
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "questions_overview_raw = read_stackoverflow_overview(tags=['python','django'],tab='Frequent',pages=2)\n",
        "#type(questions_overview_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21u58D6xwFkU",
        "colab_type": "text"
      },
      "source": [
        "#Transformação do dado cru coletado do Stackoverflow em dataframe\n",
        "**questions_overview(questions_overview_raw)**\n",
        "\n",
        "O dataframe deve conter a visão geral das perguntas do stackoverflow, com:\n",
        "\n",
        "* link\n",
        "* brief_description\n",
        "* votes\n",
        "* views"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fj83JnZmFtH",
        "colab_type": "text"
      },
      "source": [
        "###Análise do padrão da página HTML para captura de informações relevantes:\n",
        "\n",
        "Em \"question-summary\", temos as seguintes informações relevantes:\n",
        "\n",
        "1.   class = statscontainer, com:\n",
        "*   Número de votos em class=\"vote-count-post \"\n",
        ">```<span class=\"vote-count-post high-scored-post\"><strong>2473</strong></span>```\n",
        "\n",
        "*   Número de respostas aceitas em class=\"status answered-accepted\" \n",
        ">```<div class=\"status answered-accepted\"><strong>23</strong>answers</div>```\n",
        "\n",
        "*   Conteúdo e Title contendo quantidade de views em class=\"views supernova\" \n",
        ">```<div class=\"views supernova\" title=\"307,292 views\">307k views</div>```\n",
        "\n",
        "2.   class = summary, com:\n",
        "* class=\"question-hyperlink\" contendo em *href* parte do link para compor link de acesso à página detalhada da pergunta e Título da pergunta\n",
        ">``` <a href=\"/questions/15112125/how-to-test-multiple-variables-against-a-value\" class=\"question-hyperlink\">How to test multiple variables against a value?</a>```\n",
        "\n",
        "*   Breve resumo em class=\"excerpt\"\n",
        ">```<div class=\"excerpt\"> brief description of the question ...</div>```\n",
        "\n",
        "*   Tags em class=\"post-tag\"\n",
        ">```<a href=\"/questions/tagged/python\" class=\"post-tag\" title=\"show questions tagged 'python'\" rel=\"tag\">python</a>```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEQEhGyPCfX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def questions_overview(questions_overview_raw):\n",
        "  questions_overview = { 'questions':[]}\n",
        "\n",
        "  for soups in questions_overview_raw:\n",
        "    for soup in soups:\n",
        "      title = soup.select_one('.question-hyperlink').getText()\n",
        "      link = 'https://stackoverflow.com'+soup.select_one('.question-hyperlink').get('href')\n",
        "      summary = soup.select_one('.excerpt').getText()\n",
        "      vote_count =  soup.select_one('.vote-count-post').getText()\n",
        "      answers_count = soup.select_one('.answered-accepted')\n",
        "      answers_count = re.sub('\\D','',answers_count.getText('')) if answers_count else '0'\n",
        "      views =  re.sub('views','',soup.select_one('.views').attrs['title'])\n",
        "      views = re.sub(',','',views)\n",
        "      tags = []\n",
        "      for tag in soup.select('.post-tag'): tags.append(tag.getText())\n",
        "\n",
        "      questions_overview['questions'].append({\n",
        "          'title': title,\n",
        "          'link': link,\n",
        "          'summary': summary,\n",
        "          'vote_count': int(vote_count),\n",
        "          'answers_count': int(answers_count),\n",
        "          'views': int(views),\n",
        "          'tags': tags,\n",
        "          'full_question': '',\n",
        "          'best_answer': '',\n",
        "      })\n",
        "\n",
        "  questions_df = pd.DataFrame(questions_overview['questions'])\n",
        "  \n",
        "  return questions_df"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPT8KArDl4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9dc45e4-1fdd-4a17-e462-6cbc70c598a9",
        "tags": []
      },
      "source": [
        "questions_df = questions_overview(questions_overview_raw)\n",
        "#type(questions_df)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "feA0REYGN3cg"
      },
      "source": [
        "#Exemplos de como acessar a informação no dataframe:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "colab_type": "code",
        "id": "YhPPA_TmYceK",
        "outputId": "5495d8d7-1bef-407f-c954-26d4c19ca3a9",
        "tags": []
      },
      "source": [
        "print('Lista com links:\\n',questions_df['link'][0:3])\n",
        "print('\\n Acesso a dados de um link específico\\n--- Link: ',questions_df['link'][0])\n",
        "print('\\n--- Título: ', questions_df['title'][0])\n",
        "print('\\n--- Breve Descrição: ', questions_df['summary'][0])\n",
        "print('\\n--- Contagem de votos: ', questions_df['vote_count'][0])\n",
        "print('\\n--- Contagem de respostas: ', questions_df['answers_count'][0])\n",
        "print('\\n--- Contagem de visualizações: ', questions_df['views'][0])\n",
        "print('\\n--- Lista como tags: ', questions_df['tags'][0])\n",
        "questions_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rFjl4HKL2is",
        "colab_type": "text"
      },
      "source": [
        "Próximos passos:\n",
        "\n",
        "\n",
        "1.   Enriquecer questions_df com a informação detalhada da pergunta e conteúdo da resposta com melhor avaliação\n",
        "2.   Limpar dados em questions_df para remover caracteres irrelevantes, como: \\n, \\t, artigos, pronomes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pncNTKawYOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_question_detail(questions_df):\n",
        "  \n",
        "  idx = 0\n",
        "  for link in questions_df['link']:\n",
        "    question = []\n",
        "    answer = []\n",
        "    try:\n",
        "      request = requests.get(link)\n",
        "      request.raise_for_status()\n",
        "      try:\n",
        "        soup = bs(request.text, 'html.parser')\n",
        "        questions_df['full_question'][idx] = soup.find(\"div\", {\"id\": \"question\"}).select_one('.post-text').getText()\n",
        "        questions_df['best_answer'][idx] = soup.find(\"div\", {\"id\": \"answers\"}).select_one('.post-text').getText()\n",
        "\n",
        "      except: \n",
        "        print (\"Could not transform to soup object by selecting\")\n",
        "\n",
        "    except HTTPError:\n",
        "      print (\"Could not download page\")\n",
        "\n",
        "    idx += 1\n",
        "\n",
        "    sleep(0.05)\n",
        "\n",
        "  return questions_df"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCsMerWlwhjU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "ca1ec92b-1fa9-41ac-bf90-c9e99905c553",
        "tags": []
      },
      "source": [
        "questions_df = read_question_detail(questions_df)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "questions_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove todas as pontuações e retorna lista de palavras\n",
        "def clean_text (text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) #remove todas as pontuações: '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "    text = text.replace('\\n',' ').strip() \n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "#redução das palavras para sua raiz (stemming), remoção de stopwords e palavras com menos de 2 caracteres, e criação do vocabulário com a quantidade de ocorrência de cada palavra em todos os documentos\n",
        "\n",
        "def stackoverflow_vocabulary(questions_df):\n",
        "    docs_stem_words = []\n",
        "    vocabulary = {}\n",
        "    stop_words = stopwords.words('english')\n",
        "    #stop_words.append(['could', 'would', 'might', 'can', 'should'])\n",
        "    snowball_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "    for index in range(len(questions_df)):\n",
        "        text = questions_df['title'][index] + questions_df['full_question'][index] + questions_df['best_answer'][index] \n",
        "        tokentext = word_tokenize(clean_text(text))\n",
        "        stem_words  = [snowball_stemmer.stem(word) for word in tokentext if not word in stop_words and len(word) > 2 and word not in string.punctuation]\n",
        "        docs_stem_words.append(stem_words)\n",
        "\n",
        "        #Inicializa vocabulário sem repetição de palavras\n",
        "        for word in stem_words:\n",
        "            vocabulary[word] = 0\n",
        "\n",
        "    #Contabiliza ocorrência de cada palavra em todos os documentos\n",
        "    for words in docs_stem_words:\n",
        "        for word in words:\n",
        "            vocabulary[word] += 1\n",
        "    \n",
        "    return vocabulary, docs_stem_words\n",
        "\n",
        "vocabulary, docs_stem_words = stackoverflow_vocabulary(questions_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Criar índice invertido para viabilizar buscas\n",
        "def create_InvertedIndex(vocabulary, docs_stem_words): \n",
        "    invertedList = dict()\n",
        "    for term in vocabulary:\n",
        "        invertedList[term] = list()\n",
        "        index = 0\n",
        "        for stem_words in docs_stem_words:\n",
        "            frequencia = 0\n",
        "            for word in stem_words:\n",
        "                if word == term:\n",
        "                    frequencia += 1\n",
        "            if frequencia > 0:\n",
        "                invertedList[term].append([index, frequencia])\n",
        "            index += 1\n",
        "            invertedList[term].sort(key=itemgetter(1), reverse=True)\n",
        "\n",
        "    # Serialize data into file:\n",
        "    json.dump(invertedList, open( \"stackoverflow_InvertedIndex.json\", 'w' ) )\n",
        "\n",
        "    return #invertedList\n",
        "\n",
        "#invertedList = create_InvertedIndex(vocabulary, docs_stem_words)\n",
        "create_InvertedIndex(vocabulary, docs_stem_words)\n",
        "\n",
        "# Read data from file:\n",
        "invertedList = json.load( open( \"stackoverflow_InvertedIndex.json\" ) )\n",
        "#invertedList.items()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "invertedList['python'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_stemming_docs(documents):\n",
        "    snowball_stemmer = SnowballStemmer(\"english\")\n",
        "    stop_words = stopwords.words('english')\n",
        "    #stop_words.append('could', 'would', 'might', 'can', 'should')\n",
        "    tokens = sum([word_tokenize(clean_text(document)) for document in documents], [])\n",
        "    stem_words  = [snowball_stemmer.stem(word) for word in tokens if not word in stop_words and len(word) > 2 and word not in string.punctuation]\n",
        "\n",
        "    return stem_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simple_lookup_query(query, invertedList):\n",
        "    terms = simple_stemming_docs([query])\n",
        "\n",
        "    docs_index = {}\n",
        "\n",
        "    for term in terms:\n",
        "        if term in invertedList.keys():\n",
        "            docs_index[term] = [index[0] for index in invertedList[term]]\n",
        "        else:\n",
        "            docs_index['missingTerm'] = None\n",
        "\n",
        "    return docs_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "searchTerms = input(\"Digite os termos de busca: \")\n",
        "docs_index = simple_lookup_query(searchTerms,invertedList)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "docs_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0       key: test                 value: [69, 13, 40, 34, 36, 52, 55, 67, 73, 79]\n\nForam encontrados  10  documentos com o termo de busca...\n\n0    https://stackoverflow.com/questions/7933596/dj...\n1    https://stackoverflow.com/questions/604266/dja...\n2    https://stackoverflow.com/questions/8618068/dj...\n3    https://stackoverflow.com/questions/1308386/pr...\n4    https://stackoverflow.com/questions/2268417/ex...\nName: link, dtype: object\n"
        }
      ],
      "source": [
        "def print_search_result(docs_index, questions_df, operator='OR', num_results = 5):\n",
        "    for i, (k, v) in enumerate(docs_index.items()):\n",
        "        print(\"{:<8}key: {:<20} value: {}\".format(i, k, v))\n",
        "\n",
        "    print()\n",
        "\n",
        "    resultList=[lista[1] for lista in docs_index.items()]\n",
        "\n",
        "    responseSet = []\n",
        "\n",
        "    if operator == 'AND' and 'missingTerm' in docs_index.keys():\n",
        "        resultList = []\n",
        "    elif 'missingTerm' in docs_index.keys():\n",
        "        resultList.remove(None)\n",
        "\n",
        "    if len(resultList) == 1:\n",
        "        responseSet = resultList[0]\n",
        "\n",
        "    #Realiza a interseção entre os conjuntos\n",
        "    for i in range(len(resultList)-1):\n",
        "        #Operador AND\n",
        "        if operator == 'AND':\n",
        "            responseSet.append(list(set(resultList[i]).intersection(resultList[i+1])))\n",
        "        else:\n",
        "            #Operador OR\n",
        "            responseSet.append(list(set(resultList[i]).union(resultList[i+1])))\n",
        "\n",
        "    print(\"Foram encontrados \", len(np.unique(responseSet)), \" documentos com o termo de busca...\")\n",
        "    print()\n",
        "\n",
        "    #Monta o Resultado\n",
        "    results = pd.DataFrame(columns=['link','terms','question'])\n",
        "    idx=0\n",
        "    for doc_idx in np.unique(responseSet):\n",
        "        results.set_value(idx,col=['link'], value = questions_df['link'][doc_idx])\n",
        "        idx += 1\n",
        "        \n",
        "        #for term in docs_index.keys():\n",
        "        #    results['terms'][idx] = ?\n",
        "        #    documento = documento.replace(term, \"\\033[48;5;0m\\033[38;5;226m {term} \\033[0;0m\".format(term=term))        \n",
        "        #lista.append(str(str(doc+1) + \" - \" + documento))\n",
        "        \n",
        "    #Exibe o Resultado\n",
        "    print(results['link'][:num_results])\n",
        "\n",
        "    return\n",
        "\n",
        "print_search_result(docs_index,questions_df,'OR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# create functions for TD-IDF / BM25\n",
        "import math\n",
        "from textblob import TextBlob as tb\n",
        "\n",
        "def tf(word, doc):\n",
        "    return doc.count(word) / len(doc)\n",
        "\n",
        "def n_containing(word, doclist):\n",
        "    return sum(1 for doc in doclist if word in doc)\n",
        "\n",
        "def idf(word, doclist):\n",
        "    return math.log(len(doclist) / (0.01 + n_containing(word, doclist)))\n",
        "\n",
        "def tfidf(word, doc, doclist):\n",
        "    return (tf(word, doc) * idf(word, doclist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF\n",
        "\n",
        "worddic = {}\n",
        "\n",
        "for index in range(len(set_stem_words)):\n",
        "    for word in wordsunique:\n",
        "        if word in set_stem_words[0][index]:\n",
        "            word = str(word)\n",
        "            positions = list(np.where(np.array(set_stem_words[0][index]) == word)[0])\n",
        "            idfs = tfidf(word,set_stem_words[0][index],set_stem_words)\n",
        "\n",
        "            try:\n",
        "                worddic[word] = [index,positions,idfs]\n",
        "            except:\n",
        "                worddic[word] = []\n",
        "                worddic[word] = [index,positions,idfs]\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "worddic['instal']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "print(questions_df['full_question'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "print(questions_df['best_answer'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "tfidfvectorizer.fit(['django'])\n",
        "vectortfidf = tfidfvectorizer.transform(['django'])\n",
        "# summarize encoded vector\n",
        "print(vectortfidf.shape)\n",
        "print(type(vectortfidf))\n",
        "print(vectortfidf.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer #TF\n",
        "\n",
        "text = ' '.join([word for word in keywords])\n",
        "print(text)\n",
        "\n",
        "set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit([text])\n",
        "vector = vectorizer.transform([text])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "\n",
        "#Vetoriza um texto novo\n",
        "text2 = [\"How can I install install install install  install Django? No success success success with django so far... django django\"]\n",
        "vector2 = vectorizer.transform(text2)\n",
        "print(vector2.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #TF-IDF\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "tfidfvectorizer.fit([text])\n",
        "vectortfidf = tfidfvectorizer.transform([text])\n",
        "# summarize encoded vector\n",
        "print(vectortfidf.shape)\n",
        "print(type(vectortfidf))\n",
        "print(vectortfidf.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "\n",
        "#Vetoriza um texto novo\n",
        "vectortfidf2 = tfidfvectorizer.transform(text2)\n",
        "print(vectortfidf2.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(\"TF \", cosine_similarity(vector, vector2))\n",
        "print(\"TF-IDF: \", cosine_similarity(vectortfidf, vectortfidf2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "questions_overview_raw = read_stackoverflow_overview(tags=['python','django'],tab='Frequent',pages=2)\n",
        "type(questions_overview_raw)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Read_Stackoverflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmUrBHkx90se+IXEtbfA+9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}